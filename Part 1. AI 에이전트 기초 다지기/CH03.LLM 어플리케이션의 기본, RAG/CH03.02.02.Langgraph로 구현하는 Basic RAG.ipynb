{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8671dbf4",
   "metadata": {},
   "source": [
    "# ğŸ¦œğŸ•¸ï¸ LangGraph ê¸°ë°˜ RAG ì‹¤ìŠµ\n",
    "\n",
    "ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” **LangGraph**ë¥¼ í™œìš©í•˜ì—¬ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì„ êµ¬ì¶•í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "> ğŸ“¢ **LangChain RAG vs LangGraph RAG**\n",
    "> \n",
    "> - **LangChain RAG**ëŠ” ì²´ì¸ ê¸°ë°˜ìœ¼ë¡œ ìˆœì°¨ì ì¸ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "> - **LangGraph RAG**ëŠ” **ê·¸ë˜í”„ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°**ë¡œ ë” ë³µì¡í•œ ë¡œì§ê³¼ ì¡°ê±´ë¶€ ë¶„ê¸°ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "> - LangGraphëŠ” **ìƒíƒœ(State) ê´€ë¦¬**ê°€ ëª…í™•í•˜ì—¬ ë””ë²„ê¹…ê³¼ í™•ì¥ì´ ìš©ì´í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b030a60",
   "metadata": {},
   "source": [
    "## ğŸ“‹ ëª©ì°¨\n",
    "\n",
    "### Part A: Basic RAG\n",
    "1. [í™˜ê²½ ì„¤ì •](#1-í™˜ê²½-ì„¤ì •)\n",
    "2. [ë¬¸ì„œ ë¡œë”© ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±](#2-ë¬¸ì„œ-ë¡œë”©-ë°-ë²¡í„°-ìŠ¤í† ì–´-ìƒì„±)\n",
    "3. [RAG ì²´ì¸ ì„¤ì •](#3-rag-ì²´ì¸-ì„¤ì •)\n",
    "4. [LangGraph ìƒíƒœ ì •ì˜](#4-langgraph-ìƒíƒœ-ì •ì˜)\n",
    "5. [ê·¸ë˜í”„ ë…¸ë“œ í•¨ìˆ˜ ì •ì˜](#5-ê·¸ë˜í”„-ë…¸ë“œ-í•¨ìˆ˜-ì •ì˜)\n",
    "6. [ê·¸ë˜í”„ ìƒì„± ë° ì»´íŒŒì¼](#6-ê·¸ë˜í”„-ìƒì„±-ë°-ì»´íŒŒì¼)\n",
    "7. [ê·¸ë˜í”„ ì‹œê°í™”](#7-ê·¸ë˜í”„-ì‹œê°í™”)\n",
    "8. [RAG ì‹¤í–‰ í…ŒìŠ¤íŠ¸](#8-rag-ì‹¤í–‰-í…ŒìŠ¤íŠ¸)\n",
    "\n",
    "### Part B: Agentic RAG (ì‹¬í™”)\n",
    "9. [Agentic RAG ê°œìš”](#9-agentic-rag-ê°œìš”)\n",
    "10. [ì›¹ ë¬¸ì„œ ì „ì²˜ë¦¬](#10-ì›¹-ë¬¸ì„œ-ì „ì²˜ë¦¬)\n",
    "11. [Retriever Tool ìƒì„±](#11-retriever-tool-ìƒì„±)\n",
    "12. [ì¿¼ë¦¬ ìƒì„± ë˜ëŠ” ì‘ë‹µ](#12-ì¿¼ë¦¬-ìƒì„±-ë˜ëŠ”-ì‘ë‹µ)\n",
    "13. [ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€](#13-ë¬¸ì„œ-ê´€ë ¨ì„±-í‰ê°€)\n",
    "14. [ì§ˆë¬¸ ì¬ì‘ì„±](#14-ì§ˆë¬¸-ì¬ì‘ì„±)\n",
    "15. [ë‹µë³€ ìƒì„±](#15-ë‹µë³€-ìƒì„±)\n",
    "16. [Agentic RAG ê·¸ë˜í”„ ì¡°ë¦½](#16-agentic-rag-ê·¸ë˜í”„-ì¡°ë¦½)\n",
    "17. [Agentic RAG ì‹¤í–‰](#17-agentic-rag-ì‹¤í–‰)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a622a",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. í™˜ê²½ ì„¤ì •\n",
    "\n",
    "ë¨¼ì € í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ì„ ì„¤ì¹˜í•˜ê³  í™˜ê²½ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5fd258",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install -qU langchain langchain-openai langchain-community langgraph langchain-mcp-adapters mcp chromadb tiktoken bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade94327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API í‚¤ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì— ì—†ìœ¼ë©´ ì§ì ‘ ì…ë ¥)\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7fb404",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. ë¬¸ì„œ ë¡œë”© ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "\n",
    "PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³ , í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•œ ë’¤ ChromaDB ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ì£¼ìš” ì»´í¬ë„ŒíŠ¸:\n",
    "- **PyPDFLoader**: PDF ë¬¸ì„œ ë¡œë”©\n",
    "- **RecursiveCharacterTextSplitter**: í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  (tiktoken ì¸ì½”ë” ì‚¬ìš©)\n",
    "- **Chroma**: ë²¡í„° ì €ì¥ì†Œ (ë¡œì»¬ ì¸ë©”ëª¨ë¦¬)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68611aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ ë¡œë“œ ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ì„¤ì •\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# PDF íŒŒì¼ ê²½ë¡œ ì„¤ì • (OpenAI RAG ì‹¤ìŠµê³¼ ë™ì¼í•œ ë¬¸ì„œ ì‚¬ìš©)\n",
    "file_path = \"docs/DeepSeek_OCR_paper.pdf\"\n",
    "\n",
    "# PDF ë¡œë” ì´ˆê¸°í™” ë° ë¬¸ì„œ ë¡œë“œ\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcddead",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ğŸ“„ ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(docs)} í˜ì´ì§€\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f6003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ ë¶„í• \n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(doc_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af86e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ì´ {len(doc_splits)}ê°œì˜ ë¬¸ì„œ ì²­í¬ ìƒì„±ë¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16a5085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "# ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c958f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"deepseek ocr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62b30a4",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. RAG ì²´ì¸ ì„¤ì •\n",
    "\n",
    "LLM ëª¨ë¸ê³¼ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì„¤ì •í•˜ê³ , ê¸°ë³¸ RAG ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac6d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG ì²´ì¸ ì„¤ì •\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM ëª¨ë¸ ì„¤ì •\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "# RAG í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "     Use three sentences maximum and keep the answer concise. Answer in Korean\"\"\"),\n",
    "    (\"human\", \"Question: {question}\\n\\nContext: {context}\\n\\nAnswer:\")\n",
    "])\n",
    "\n",
    "# ë¬¸ì„œ í¬ë§·íŒ… í•¨ìˆ˜\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# RAG ì²´ì¸ ìƒì„±\n",
    "rag_chain = prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a1fa86",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. LangGraph ìƒíƒœ ì •ì˜\n",
    "\n",
    "LangGraphì—ì„œ ì‚¬ìš©í•  **ìƒíƒœ(State)**ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ìƒíƒœëŠ” ê·¸ë˜í”„ì˜ ê° ë…¸ë“œ ê°„ì— ì „ë‹¬ë˜ëŠ” ë°ì´í„° êµ¬ì¡°ì…ë‹ˆë‹¤.\n",
    "\n",
    "### GraphState êµ¬ì„±:\n",
    "- `question`: ì‚¬ìš©ì ì§ˆë¬¸\n",
    "- `documents`: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "- `generation`: ìƒì„±ëœ ë‹µë³€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecc17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph ìƒíƒœ ì •ì˜\n",
    "from typing import TypedDict, List, Annotated\n",
    "from langchain_core.documents import Document\n",
    "import operator\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"RAG ê·¸ë˜í”„ì˜ ìƒíƒœë¥¼ ì •ì˜\"\"\"\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    generation: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7de958d",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. ê·¸ë˜í”„ ë…¸ë“œ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "RAG ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì„±í•˜ëŠ” **ë…¸ë“œ(Node)** í•¨ìˆ˜ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ë…¸ë“œ ì„¤ëª…:\n",
    "- **retrieve**: ì§ˆë¬¸ì„ ë°›ì•„ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ë…¸ë“œ\n",
    "- **generate**: ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b0a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ë…¸ë“œ í•¨ìˆ˜ ì •ì˜\n",
    "def retrieve(state: GraphState) -> GraphState:\n",
    "    \"\"\"ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ\"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ì‚¬ìš©í•´ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰\n",
    "    documents = retriever.invoke(question)\n",
    "    \n",
    "    return {\"question\": question, \"documents\": documents, \"generation\": \"\"}\n",
    "\n",
    "\n",
    "def generate(state: GraphState) -> GraphState:\n",
    "    \"\"\"ë‹µë³€ ìƒì„± ë…¸ë“œ\"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # ë¬¸ì„œë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…\n",
    "    docs_txt = format_docs(documents)\n",
    "    \n",
    "    # RAG ì²´ì¸ì„ ì‚¬ìš©í•´ ë‹µë³€ ìƒì„±\n",
    "    generation = rag_chain.invoke({\"context\": docs_txt, \"question\": question})\n",
    "    \n",
    "    return {\"question\": question, \"documents\": documents, \"generation\": generation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3226853c",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. ê·¸ë˜í”„ ìƒì„± ë° ì»´íŒŒì¼\n",
    "\n",
    "ë…¸ë“œë“¤ì„ ì—°ê²°í•˜ì—¬ **StateGraph**ë¥¼ ìƒì„±í•˜ê³  ì»´íŒŒì¼í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ê·¸ë˜í”„ êµ¬ì¡°:\n",
    "```\n",
    "START â†’ retrieve â†’ generate â†’ END\n",
    "```\n",
    "\n",
    "ì´ ë‹¨ìˆœí•œ êµ¬ì¡°ê°€ ê¸°ë³¸ RAGì˜ í•µì‹¬ì…ë‹ˆë‹¤. LangGraphë¥¼ ì‚¬ìš©í•˜ë©´ ì´í›„ì— ì¡°ê±´ë¶€ ë¶„ê¸°, ë£¨í”„ ë“± ë³µì¡í•œ ë¡œì§ì„ ì‰½ê²Œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee596983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph ê·¸ë˜í”„ ìƒì„± ë° ì»´íŒŒì¼\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# ë…¸ë“œ ì¶”ê°€\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "# ì—£ì§€ ì—°ê²°\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"RAG ê·¸ë˜í”„ê°€ ì„±ê³µì ìœ¼ë¡œ ì»´íŒŒì¼ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7f75f3",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. ê·¸ë˜í”„ ì‹œê°í™”\n",
    "\n",
    "ì»´íŒŒì¼ëœ ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•˜ì—¬ ì›Œí¬í”Œë¡œìš°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6560ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ì‹œê°í™”\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"ì‹œê°í™” ì˜¤ë¥˜: {e}\")\n",
    "    print(\"ê·¸ë˜í”„ êµ¬ì¡°: START -> retrieve -> generate -> END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a11375",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. RAG ì‹¤í–‰ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "êµ¬ì¶•í•œ RAG ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•˜ì—¬ ì§ˆì˜ì‘ë‹µì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "### í…ŒìŠ¤íŠ¸ ë°©ë²•:\n",
    "1. **invoke()**: ë™ê¸° ì‹¤í–‰ - ì „ì²´ ê²°ê³¼ë¥¼ í•œ ë²ˆì— ë°˜í™˜\n",
    "2. **stream()**: ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ - ê° ë…¸ë“œ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ë°˜í™˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02538693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG ê·¸ë˜í”„ ì‹¤í–‰ í…ŒìŠ¤íŠ¸\n",
    "question = \"Deepseek OCRì´ ë­ì•¼?\"\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "result = app.invoke({\"question\": question})\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"ì§ˆë¬¸: {result['question']}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\në‹µë³€:\\n{result['generation']}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nì°¸ì¡° ë¬¸ì„œ ìˆ˜: {len(result['documents'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0abd680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰\n",
    "question = \"Omnidoc bench ê²°ê³¼ëŠ” ì–´ë•Œ?\"\n",
    "\n",
    "print(f\"ì§ˆë¬¸: {question}\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for output in app.stream({\"question\": question}):\n",
    "    for node_name, value in output.items():\n",
    "        print(f\"\\n[{node_name}] ë…¸ë“œ ì‹¤í–‰ ì™„ë£Œ\")\n",
    "        if node_name == \"generate\" and \"generation\" in value:\n",
    "            print(f\"\\në‹µë³€:\\n{value['generation']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374579b1",
   "metadata": {},
   "source": [
    "### ğŸ’¡ LangGraph RAGì˜ ì¥ì \n",
    "\n",
    "1. **ëª…í™•í•œ ìƒíƒœ ê´€ë¦¬**: TypedDictë¡œ ìƒíƒœë¥¼ ì •ì˜í•˜ì—¬ íƒ€ì… ì•ˆì •ì„± í™•ë³´\n",
    "2. **í™•ì¥ì„±**: ì¡°ê±´ë¶€ ë¼ìš°íŒ…, ë£¨í”„ ë“± ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° êµ¬í˜„ ê°€ëŠ¥\n",
    "3. **ë””ë²„ê¹… ìš©ì´**: ê° ë…¸ë“œì˜ ì…ì¶œë ¥ì„ ëª…í™•í•˜ê²Œ ì¶”ì  ê°€ëŠ¥\n",
    "4. **ì‹œê°í™”**: ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d3902",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“š ì°¸ê³  ìë£Œ\n",
    "\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174a3185",
   "metadata": {},
   "source": [
    "# ![Langgraph Hybrid RAG Tutorial](https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/langgraph-hybrid-rag-tutorial.png?w=840&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=b7626e6ae3cb94fb90a61e6fad69c8ba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6742076e",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ¤– Part B: Agentic RAG êµ¬í˜„\n",
    "\n",
    "## 9. Agentic RAG ê°œìš”\n",
    "\n",
    "ì´ì œ ë” ë°œì „ëœ í˜•íƒœì˜ **Agentic RAG**ë¥¼ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "### Agentic RAGë€?\n",
    "LLM ì—ì´ì „íŠ¸ê°€ **ê²€ìƒ‰ ì—¬ë¶€ì™€ ë°©ë²•ì„ ìŠ¤ìŠ¤ë¡œ ê²°ì •**í•˜ëŠ” RAG ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n",
    "\n",
    "### ì£¼ìš” íŠ¹ì§•:\n",
    "- LLMì´ retriever tool í˜¸ì¶œ ì—¬ë¶€ë¥¼ **ììœ¨ì ìœ¼ë¡œ íŒë‹¨**\n",
    "- ê²€ìƒ‰ëœ ë¬¸ì„œì˜ **ê´€ë ¨ì„±ì„ í‰ê°€**í•˜ê³  í•„ìš”ì‹œ ì§ˆë¬¸ì„ ì¬ì‘ì„±\n",
    "- ì¡°ê±´ë¶€ ë¶„ê¸°ì™€ ë£¨í”„ë¥¼ í†µí•œ **ìê¸° ê°œì„ ** ê°€ëŠ¥\n",
    "\n",
    "### ê·¸ë˜í”„ êµ¬ì¡°:\n",
    "```\n",
    "START â†’ generate_query_or_respond â†’ [tool í˜¸ì¶œ?]\n",
    "                                      â”œâ”€ Yes â†’ retrieve â†’ [ê´€ë ¨ì„±?]\n",
    "                                      â”‚                     â”œâ”€ Yes â†’ generate_answer â†’ END\n",
    "                                      â”‚                     â””â”€ No â†’ rewrite_question â”€â”˜\n",
    "                                      â””â”€ No â†’ END (ì§ì ‘ ì‘ë‹µ)\n",
    "```\n",
    "\n",
    "![Agentic RAG](https://docs.langchain.com/oss/images/agentic-rag-output.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fafc91",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. ë¬¸ì„œ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded9c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF ë¬¸ì„œ ë¡œë”©\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path = \"docs/DeepSeek_OCR_paper.pdf\"\n",
    "\n",
    "pdf_loader = PyPDFLoader(pdf_path)\n",
    "docs = [pdf_loader.load()]\n",
    "print(f\"ğŸ“„ PDF ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ec90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# ë¬¸ì„œ ë¶„í•  ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "print(f\"âœ‚ï¸ ì´ {len(doc_splits)}ê°œì˜ ì²­í¬ ìƒì„±\")\n",
    "\n",
    "# ì¸ë©”ëª¨ë¦¬ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
    "\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=doc_splits, \n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "print(\"âœ… ë¬¸ì„œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb0f67",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Retriever Tool ìƒì„±\n",
    "\n",
    "`@tool` ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ë„êµ¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì—ì´ì „íŠ¸ê°€ ì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì—¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8698d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def retrieve(query: str) -> str:\n",
    "    \"\"\"DeepSeek OCR ë…¼ë¬¸ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "retriever_tool = retrieve\n",
    "print(\"âœ… Retriever Tool ìƒì„± ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e2daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë„êµ¬ í…ŒìŠ¤íŠ¸\n",
    "retriever_tool.invoke({\"query\": \"deepseek ocr ëª¨ë¸\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849479b4",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. ì¿¼ë¦¬ ìƒì„± ë˜ëŠ” ì‘ë‹µ\n",
    "\n",
    "`generate_query_or_respond` ë…¸ë“œëŠ” LLMì´ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë³´ê³  **ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨**í•©ë‹ˆë‹¤.\n",
    "- ê²€ìƒ‰ì´ í•„ìš”í•˜ë©´ â†’ retriever tool í˜¸ì¶œ\n",
    "- í•„ìš” ì—†ìœ¼ë©´ â†’ ì§ì ‘ ì‘ë‹µ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b1c5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "response_model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "def generate_query_or_respond(state: MessagesState):\n",
    "    \"\"\"LLMì´ ê²€ìƒ‰ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ì—¬ tool í˜¸ì¶œ ë˜ëŠ” ì§ì ‘ ì‘ë‹µ\"\"\"\n",
    "    response = (\n",
    "        response_model\n",
    "        .bind_tools([retriever_tool])\n",
    "        .invoke(state[\"messages\"])\n",
    "    )\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "print(\"âœ… generate_query_or_respond ë…¸ë“œ ì •ì˜ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5179632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸: ì¼ë°˜ ì¸ì‚¬ (ê²€ìƒ‰ ë¶ˆí•„ìš”)\n",
    "test_input = {\"messages\": [{\"role\": \"user\", \"content\": \"hello!\"}]}\n",
    "generate_query_or_respond(test_input)[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad2f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸: ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸ (Tool Call ë°œìƒ)\n",
    "test_input = {\"messages\": [{\"role\": \"user\", \"content\": \"DeepEncoderë€?\"}]}\n",
    "generate_query_or_respond(test_input)[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c854979f",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€\n",
    "\n",
    "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ **ê´€ë ¨ì´ ìˆëŠ”ì§€ í‰ê°€**í•˜ëŠ” ì¡°ê±´ë¶€ ì—£ì§€ì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6e9e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "GRADE_PROMPT = (\n",
    "    \"You are a grader assessing relevance of a retrieved document to a user question.\\n\"\n",
    "    \"Here is the retrieved document:\\n\\n{context}\\n\\n\"\n",
    "    \"Here is the user question: {question}\\n\"\n",
    "    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\\n\"\n",
    "    \"Give a binary score 'yes' or 'no' to indicate whether the document is relevant.\"\n",
    ")\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ë¥¼ ìœ„í•œ ì´ì§„ ì ìˆ˜\"\"\"\n",
    "    binary_score: str = Field(description=\"Relevance score: 'yes' or 'no'\")\n",
    "\n",
    "grader_model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "def grade_documents(state: MessagesState) -> Literal[\"generate_answer\", \"rewrite_question\"]:\n",
    "    \"\"\"ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ í‰ê°€\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    \n",
    "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
    "    response = grader_model.with_structured_output(GradeDocuments).invoke(\n",
    "        [{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    if response.binary_score == \"yes\":\n",
    "        print(\"---GRADE: ê´€ë ¨ ìˆìŒ---\")\n",
    "        return \"generate_answer\"\n",
    "    else:\n",
    "        print(\"---GRADE: ê´€ë ¨ ì—†ìŒ â†’ ì§ˆë¬¸ ì¬ì‘ì„±---\")\n",
    "        return \"rewrite_question\"\n",
    "\n",
    "print(\"âœ… grade_documents ì¡°ê±´ë¶€ ì—£ì§€ ì •ì˜ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d8417f",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. ì§ˆë¬¸ ì¬ì‘ì„±\n",
    "\n",
    "ê²€ìƒ‰ ê²°ê³¼ê°€ ê´€ë ¨ ì—†ì„ ê²½ìš°, **ì§ˆë¬¸ì„ ê°œì„ **í•˜ì—¬ ë‹¤ì‹œ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f40d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "REWRITE_PROMPT = (\n",
    "    \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"\n",
    "    \"Here is the initial question:\\n---\\n{question}\\n---\\n\"\n",
    "    \"Formulate an improved question:\"\n",
    ")\n",
    "\n",
    "def rewrite_question(state: MessagesState):\n",
    "    \"\"\"ì§ˆë¬¸ì„ ì¬ì‘ì„±\"\"\"\n",
    "    print(\"---REWRITE QUESTION---\")\n",
    "    question = state[\"messages\"][0].content\n",
    "    prompt = REWRITE_PROMPT.format(question=question)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": [HumanMessage(content=response.content)]}\n",
    "\n",
    "print(\"âœ… rewrite_question ë…¸ë“œ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3827f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_PROMPT = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, just say that you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "    \"Question: {question}\\nContext: {context}\"\n",
    ")\n",
    "\n",
    "def generate_answer(state: MessagesState):\n",
    "    \"\"\"ìµœì¢… ë‹µë³€ ìƒì„±\"\"\"\n",
    "    print(\"---GENERATE ANSWER---\")\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    prompt = GENERATE_PROMPT.format(question=question, context=context)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "print(\"âœ… generate_answer ë…¸ë“œ ì •ì˜ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da09b61",
   "metadata": {},
   "source": [
    "---\n",
    "## 16. Agentic RAG ê·¸ë˜í”„ ì¡°ë¦½\n",
    "\n",
    "ëª¨ë“  ë…¸ë“œì™€ ì—£ì§€ë¥¼ ì—°ê²°í•˜ì—¬ **ì™„ì „í•œ Agentic RAG ê·¸ë˜í”„**ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd57bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# Agentic RAG ê·¸ë˜í”„ ìƒì„±\n",
    "agentic_workflow = StateGraph(MessagesState)\n",
    "\n",
    "# ë…¸ë“œ ì¶”ê°€\n",
    "agentic_workflow.add_node(generate_query_or_respond)\n",
    "agentic_workflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\n",
    "agentic_workflow.add_node(rewrite_question)\n",
    "agentic_workflow.add_node(generate_answer)\n",
    "\n",
    "# ì‹œì‘ ì—£ì§€\n",
    "agentic_workflow.add_edge(START, \"generate_query_or_respond\")\n",
    "\n",
    "# ì¡°ê±´ë¶€ ì—£ì§€: Tool í˜¸ì¶œ ì—¬ë¶€ íŒë‹¨\n",
    "agentic_workflow.add_conditional_edges(\n",
    "    \"generate_query_or_respond\",\n",
    "    tools_condition,\n",
    "    {\"tools\": \"retrieve\", END: END}\n",
    ")\n",
    "\n",
    "# ì¡°ê±´ë¶€ ì—£ì§€: ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€\n",
    "agentic_workflow.add_conditional_edges(\"retrieve\", grade_documents)\n",
    "\n",
    "# ì¼ë°˜ ì—£ì§€\n",
    "agentic_workflow.add_edge(\"generate_answer\", END)\n",
    "agentic_workflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\")\n",
    "\n",
    "# ì»´íŒŒì¼\n",
    "agentic_graph = agentic_workflow.compile()\n",
    "print(\"âœ… Agentic RAG ê·¸ë˜í”„ ì»´íŒŒì¼ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c89551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agentic RAG ê·¸ë˜í”„ ì‹œê°í™”\n",
    "try:\n",
    "    display(Image(agentic_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"ì‹œê°í™” ì˜¤ë¥˜: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54de620",
   "metadata": {},
   "source": [
    "---\n",
    "## 17. Agentic RAG ì‹¤í–‰\n",
    "\n",
    "ì™„ì„±ëœ Agentic RAG ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•˜ì—¬ ì§ˆì˜ì‘ë‹µì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2882ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agentic RAG ì‹¤í–‰ í…ŒìŠ¤íŠ¸\n",
    "for chunk in agentic_graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Mistral OCRì´ ë­ì•¼?\"}]}\n",
    "):\n",
    "    for node, update in chunk.items():\n",
    "        print(f\"\\nğŸ”„ Update from node: {node}\")\n",
    "        update[\"messages\"][-1].pretty_print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
